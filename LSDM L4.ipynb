{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS CLI code for initializing cluster:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aws emr create-cluster --auto-scaling-role EMR_AutoScaling_DefaultRole --applications Name=Hive Name=JupyterHub Name=Ganglia Name=Spark --ebs-root-volume-size 10 --ec2-attributes '{\"KeyName\":\"aws-key1\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-05f8732b\",\"EmrManagedSlaveSecurityGroup\":\"sg-0314eaf4b5c5d861d\",\"EmrManagedMasterSecurityGroup\":\"sg-0314eaf4b5c5d861d\"}' --service-role EMR_DefaultRole --enable-debugging --release-label emr-5.19.0 --log-uri 's3n://aws-logs-253161286339-us-east-1/elasticmapreduce/' --name 'lsdm_cluster_ps3_vf' --instance-groups '[{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":1}]},\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Master - 1\"},{\"InstanceCount\":2,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":1}]},\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Core - 2\"}]' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-east-1 --bootstrap-actions Path=s3://lsdmghosevf/lsdm_ps3_bash.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1543175043386_0006</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-81-31.ec2.internal:20888/proxy/application_1543175043386_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-94-165.ec2.internal:8042/node/containerlogs/container_1543175043386_0006_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#Referenced: https://www.quora.com/How-would-I-decide-create-a-Spark-cluster-infrastructure-given-the-size-and-frequency-of-data-that-I-get\n",
    "\n",
    "#Explanation for cluster configuration decisions: We know that the file sizes of the three parquet data sources\n",
    "#used in this assignment are od (14.3GB), rac (2GB), and wac (787MB) -- for a combined total of 17.087GB.  In order to\n",
    "#factor in additional space for replication (2x), as well as for applications installed on the cluster like Spark, I decided to\n",
    "#separate 25% of available space on the cluster.  This means that I need (17.087Gb*2)+ .25(17.087Gb*2) = 42.718Gb.  Since\n",
    "#each m5.xlarge machine has 16Gb of memory, I meed the ceiling of 41.718GB/16GB = 3 machines.  With this in mind, I chose to use 1 master\n",
    "#node and 2 core machines for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import lower, col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkExploration\").config('spark.executor.instances', '3').config('spark.executor.memory', '16G').config('spark.executor.cores', '3').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = spark.read.option('header', 'true').option('inferSchema', 'true').parquet('s3://lsdm-emr-util/lsdm-data/lodes/od/od.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rac = spark.read.option('header', 'true').option('inferSchema', 'true').parquet('s3://lsdm-emr-util/lsdm-data/lodes/rac/rac.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wac = spark.read.option('header', 'true').option('inferSchema', 'true').parquet('s3://lsdm-emr-util/lsdm-data/lodes/wac/wac.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+----+----+----+----+----+----+----+----+----+----+----------+----+\n",
      "|      w_geocode|      h_geocode|s000|sa01|sa02|sa03|se01|se02|se03|si01|si02|si03|createdate|year|\n",
      "+---------------+---------------+----+----+----+----+----+----+----+----+----+----+----------+----+\n",
      "|271630714002025|271630712082020|   1|   0|   1|   0|   0|   1|   0|   1|   0|   0|  20160219|2012|\n",
      "+---------------+---------------+----+----+----+----+----+----+----+----+----+----+----------+----+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "od.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----------+----+\n",
      "|      h_geocode|c000|ca01|ca02|ca03|ce01|ce02|ce03|cns01|cns02|cns03|cns04|cns05|cns06|cns07|cns08|cns09|cns10|cns11|cns12|cns13|cns14|cns15|cns16|cns17|cns18|cns19|cns20|cr01|cr02|cr03|cr04|cr05|cr07|ct01|ct02|cd01|cd02|cd03|cd04|cs01|cs02|createdate|year|\n",
      "+---------------+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----------+----+\n",
      "|260010001001000|   4|   0|   4|   0|   1|   1|   2|    0|    0|    0|    0|    0|    0|    0|    1|    1|    0|    0|    0|    0|    1|    0|    1|    0|    0|    0|    0|   4|   0|   0|   0|   0|   0|   4|   0|   0|   1|   2|   1|   3|   1|  20170919|2015|\n",
      "+---------------+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----------+----+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "rac.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----------+----+\n",
      "|      w_geocode|c000|ca01|ca02|ca03|ce01|ce02|ce03|cns01|cns02|cns03|cns04|cns05|cns06|cns07|cns08|cns09|cns10|cns11|cns12|cns13|cns14|cns15|cns16|cns17|cns18|cns19|cns20|cr01|cr02|cr03|cr04|cr05|cr07|ct01|ct02|cd01|cd02|cd03|cd04|cs01|cs02|cfa01|cfa02|cfa03|cfa04|cfa05|cfs01|cfs02|cfs03|cfs04|cfs05|createdate|year|\n",
      "+---------------+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----------+----+\n",
      "|060014001001007|  32|  11|  10|  11|  12|   4|  16|    0|    0|    0|    0|    0|    0|    0|    0|    0|    2|    0|   17|    0|    0|    0|    0|    7|    0|    6|    0|  24|   1|   0|   4|   2|   1|  28|   4|   5|   5|   3|   8|   9|  23|    0|    0|    0|    0|    0|    0|    0|    0|    0|    0|  20160219|2013|\n",
      "+---------------+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----------+----+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "wac.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w_geocode', 'h_geocode', 's000', 'sa01', 'sa02', 'sa03', 'se01', 'se02', 'se03', 'si01', 'si02', 'si03', 'createdate', 'year']"
     ]
    }
   ],
   "source": [
    "od.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h_geocode', 'c000', 'ca01', 'ca02', 'ca03', 'ce01', 'ce02', 'ce03', 'cns01', 'cns02', 'cns03', 'cns04', 'cns05', 'cns06', 'cns07', 'cns08', 'cns09', 'cns10', 'cns11', 'cns12', 'cns13', 'cns14', 'cns15', 'cns16', 'cns17', 'cns18', 'cns19', 'cns20', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr07', 'ct01', 'ct02', 'cd01', 'cd02', 'cd03', 'cd04', 'cs01', 'cs02', 'createdate', 'year']"
     ]
    }
   ],
   "source": [
    "rac.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w_geocode', 'c000', 'ca01', 'ca02', 'ca03', 'ce01', 'ce02', 'ce03', 'cns01', 'cns02', 'cns03', 'cns04', 'cns05', 'cns06', 'cns07', 'cns08', 'cns09', 'cns10', 'cns11', 'cns12', 'cns13', 'cns14', 'cns15', 'cns16', 'cns17', 'cns18', 'cns19', 'cns20', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr07', 'ct01', 'ct02', 'cd01', 'cd02', 'cd03', 'cd04', 'cs01', 'cs02', 'cfa01', 'cfa02', 'cfa03', 'cfa04', 'cfa05', 'cfs01', 'cfs02', 'cfs03', 'cfs04', 'cfs05', 'createdate', 'year']"
     ]
    }
   ],
   "source": [
    "wac.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "od = od.withColumn('w_tract', od.w_geocode.substr(1,11)) \n",
    "od = od.withColumn('h_tract', od.h_geocode.substr(1,11)) \n",
    "rac = rac.withColumn('h_tract', rac.h_geocode.substr(0,11))\n",
    "wac = wac.withColumn('w_tract', wac.w_geocode.substr(0,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "od.createOrReplaceTempView(\"od\")\n",
    "rac.createOrReplaceTempView(\"rac\")\n",
    "wac.createOrReplaceTempView(\"wac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = spark.sql(\"SELECT * from od left join wac on wac.w_tract = od.w_tract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView(\"merged_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = spark.sql(\"SELECT * from merged_df left join rac on rac.h_tract = merged_df.h_tract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w_geocode', 'h_geocode', 's000', 'sa01', 'sa02', 'sa03', 'se01', 'se02', 'se03', 'si01', 'si02', 'si03', 'createdate', 'year', 'w_tract', 'h_tract', 'w_geocode', 'c000', 'ca01', 'ca02', 'ca03', 'ce01', 'ce02', 'ce03', 'cns01', 'cns02', 'cns03', 'cns04', 'cns05', 'cns06', 'cns07', 'cns08', 'cns09', 'cns10', 'cns11', 'cns12', 'cns13', 'cns14', 'cns15', 'cns16', 'cns17', 'cns18', 'cns19', 'cns20', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr07', 'ct01', 'ct02', 'cd01', 'cd02', 'cd03', 'cd04', 'cs01', 'cs02', 'cfa01', 'cfa02', 'cfa03', 'cfa04', 'cfa05', 'cfs01', 'cfs02', 'cfs03', 'cfs04', 'cfs05', 'createdate', 'year', 'w_tract', 'h_geocode', 'c000', 'ca01', 'ca02', 'ca03', 'ce01', 'ce02', 'ce03', 'cns01', 'cns02', 'cns03', 'cns04', 'cns05', 'cns06', 'cns07', 'cns08', 'cns09', 'cns10', 'cns11', 'cns12', 'cns13', 'cns14', 'cns15', 'cns16', 'cns17', 'cns18', 'cns19', 'cns20', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr07', 'ct01', 'ct02', 'cd01', 'cd02', 'cd03', 'cd04', 'cs01', 'cs02', 'createdate', 'year', 'h_tract']"
     ]
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.8, .2]\n",
    "seed = 30\n",
    "training_set, test_set = final_df.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['cns01', 'cns02', 'cns03', 'cns04', 'cns05', 'cns06', 'cns07', 'cns08', 'cns09', 'cns10', 'cns11', 'cns12', 'cns13', 'cns14']\n",
    "label = 's000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(h_geocode='260010001001000', c000=4, ca01=0, ca02=4, ca03=0, ce01=1, ce02=1, ce03=2, cns01=0, cns02=0, cns03=0, cns04=0, cns05=0, cns06=0, cns07=0, cns08=1, cns09=1, cns10=0, cns11=0, cns12=0, cns13=0, cns14=1, cns15=0, cns16=1, cns17=0, cns18=0, cns19=0, cns20=0, cr01=4, cr02=0, cr03=0, cr04=0, cr05=0, cr07=0, ct01=4, ct02=0, cd01=0, cd02=1, cd03=2, cd04=1, cs01=3, cs02=1, createdate='20170919', year=2015, h_tract='26001000100', features=SparseVector(14, {7: 1.0, 8: 1.0, 13: 1.0})), Row(h_geocode='260010001001004', c000=2, ca01=0, ca02=2, ca03=0, ce01=1, ce02=1, ce03=0, cns01=0, cns02=0, cns03=0, cns04=0, cns05=0, cns06=0, cns07=0, cns08=0, cns09=0, cns10=1, cns11=0, cns12=0, cns13=0, cns14=0, cns15=0, cns16=0, cns17=0, cns18=1, cns19=0, cns20=0, cr01=2, cr02=0, cr03=0, cr04=0, cr05=0, cr07=0, ct01=2, ct02=0, cd01=1, cd02=1, cd03=0, cd04=0, cs01=1, cs02=1, createdate='20170919', year=2015, h_tract='26001000100', features=SparseVector(14, {9: 1.0})), Row(h_geocode='260010001001005', c000=1, ca01=0, ca02=0, ca03=1, ce01=0, ce02=0, ce03=1, cns01=0, cns02=0, cns03=0, cns04=0, cns05=1, cns06=0, cns07=0, cns08=0, cns09=0, cns10=0, cns11=0, cns12=0, cns13=0, cns14=0, cns15=0, cns16=0, cns17=0, cns18=0, cns19=0, cns20=0, cr01=1, cr02=0, cr03=0, cr04=0, cr05=0, cr07=0, ct01=1, ct02=0, cd01=0, cd02=1, cd03=0, cd04=0, cs01=0, cs02=1, createdate='20170919', year=2015, h_tract='26001000100', features=SparseVector(14, {4: 1.0})), Row(h_geocode='260010001001008', c000=3, ca01=1, ca02=1, ca03=1, ce01=0, ce02=0, ce03=3, cns01=0, cns02=0, cns03=0, cns04=1, cns05=1, cns06=0, cns07=0, cns08=0, cns09=0, cns10=0, cns11=0, cns12=0, cns13=0, cns14=0, cns15=0, cns16=1, cns17=0, cns18=0, cns19=0, cns20=0, cr01=2, cr02=0, cr03=1, cr04=0, cr05=0, cr07=0, ct01=3, ct02=0, cd01=0, cd02=2, cd03=0, cd04=0, cs01=1, cs02=2, createdate='20170919', year=2015, h_tract='26001000100', features=SparseVector(14, {3: 1.0, 4: 1.0})), Row(h_geocode='260010001001016', c000=1, ca01=1, ca02=0, ca03=0, ce01=1, ce02=0, ce03=0, cns01=0, cns02=0, cns03=0, cns04=0, cns05=0, cns06=0, cns07=0, cns08=0, cns09=0, cns10=0, cns11=0, cns12=0, cns13=0, cns14=0, cns15=0, cns16=0, cns17=1, cns18=0, cns19=0, cns20=0, cr01=1, cr02=0, cr03=0, cr04=0, cr05=0, cr07=0, ct01=1, ct02=0, cd01=0, cd02=0, cd03=0, cd04=0, cs01=0, cs02=1, createdate='20170919', year=2015, h_tract='26001000100', features=SparseVector(14, {}))]"
     ]
    }
   ],
   "source": [
    "vAssemble = VectorAssembler(inputCols = feature_cols, outputCol = 'features')\n",
    "rac_a = vAssemble.transform(rac)\n",
    "rac_a.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot import name 'LinearRegression'\n",
      "Traceback (most recent call last):\n",
      "ImportError: cannot import name 'LinearRegression'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=label, featuresCol = feature_cols)\n",
    "paramGrid_lr = ParamGridBuilder().addGrid(lr.solver, [\"l-bfgs\", \"normal\"]).build()\n",
    "evaluator_lr = RegressionEvaluator(predictionCol = \"prediction_lr\", labelCol = label, metricName = \"rmse\")\n",
    "cv_lr = CrossValidator(estimator = lr, estimatorParamMaps = paramGrid_lr, evaluator = evaluator_lr, numFolds = 5)\n",
    "cvModel_lr = cv.fit(training_set)\n",
    "score_lr = cvModel_lr.predict(training_set)\n",
    "cvModel_lr.save('s3n://aws-logs-253161286339-us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot import name 'RandomForestRegressor'\n",
      "Traceback (most recent call last):\n",
      "ImportError: cannot import name 'RandomForestRegressor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(labelCol =label, featuresCol = feature_cols)\n",
    "paramGrid_rf = (ParamGridBuilder().addGrid(lr.maxDepth, [3,5,7]).addGrid(lr.featureSubsetStrategy, [\"all\",\"sqrt\", \"log2\"]).addGrid(lr.numTrees, [50,200]).build())\n",
    "evaluator_rf = RegressionEvaluator(predictionCol = \"prediction_rf\", labelCol = label, metricName = \"rmse\")\n",
    "cv_rf = CrossValidator(estimator = rf, estimatorParamMaps = paramGrid_rf, evaluator = evaluator_rf, numFolds = 5)\n",
    "cvModel_rf = cv_rf.fit(training_set)\n",
    "score_rf = cvModel_rf.predict(training_set)\n",
    "cvModel_rf.save('s3n://aws-logs-253161286339-us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot import name 'GradientBoostingRegressor'\n",
      "Traceback (most recent call last):\n",
      "ImportError: cannot import name 'GradientBoostingRegressor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(labelCol =label, featuresCol = feature_cols)\n",
    "paramGrid_gbr = (ParamGridBuilder().addGrid().build())\n",
    "evaluator_gbr = RegressionEvaluator(predictionCol = \"prediction_gbr\", labelCol = label, metricName = \"rmse\")\n",
    "cv_gbr = CrossValidator(estimator = gbr, estimatorParamMaps = paramGrid_gbr, evaluator = evaluator_gbr, numFolds = 5)\n",
    "cvModel_gbr = cv_gbr.fit(training_set)\n",
    "score_gbr = cvModel_gbr.predict(training_set)\n",
    "cvModel_gbr.save('s3n://aws-logs-253161286339-us-east-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
