{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import requests\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2 Syntax\n",
    "\n",
    "Set up EC2 instance successfully using: \n",
    "\n",
    "aws ec2 run-instances --image-id ami-0ac019f4fcb7cb7e6 --count 1 --instance-type t2.xlarge --key-name aws-key1 --security-group-ids sg-0fea89a4bb6f8a4b5 --user-data file://CoreNLP.sh\n",
    "\n",
    "then verified that corenlp was properly installed in the ec2 instance with this in browser: \n",
    "http://ec2-35-173-202-124.compute-1.amazonaws.com:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "#referenced: https://www.quora.com/How-can-I-extract-only-text-data-from-HTML-pages, https://github.com/smilli/py-corenlp\n",
    "\n",
    "html1 = urllib.request.urlopen('https://www.nytimes.com/2018/09/05/opinion/trump-white-house-anonymous-resistance.html?module=inline')\n",
    "html2 = urllib.request.urlopen('https://www.nytimes.com/2018/10/20/opinion/sunday/nafta-mexico-trump-ambassador.html?rref=collection%2Fsectioncollection%2Fopinion&action=click&contentCollection=opinion&region=rank&module=package&version=highlights&contentPlacement=7&pgtype=sectionfront')\n",
    "\n",
    "'''\n",
    "Gets all text from articles in text stream.\n",
    "'''\n",
    "soup = BeautifulSoup(html1, \"lxml\")\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")\n",
    "\n",
    "data1 = soup.findAll(text=True)\n",
    "data2 = soup2.findAll(text=True)\n",
    "\n",
    "'''\n",
    "Function that returns true for relevant text from scraped articles.\n",
    "'''\n",
    "def find_relevant_text(data_input):\n",
    "    return (not data_input.parent.name in ['style', 'script', '[document]', 'head', 'title'] and not re.match('<!--.*-->', str(data_input.encode('utf-8'))))\n",
    "\n",
    "'''\n",
    "Function that filters all text to relevant text, converts filtered object to a list, and puts it in the queue.\n",
    "'''\n",
    "def list_from_text(data, q):\n",
    "    full_text = filter(find_relevant_text, data)\n",
    "    text = list(full_text)\n",
    "    q.put(text)\n",
    "        \n",
    "'''\n",
    "Function that retrieves items from queue, converts to string, and produces \n",
    "sentiment analysis results using StanfordCoreNLP and the running EC2 instance.\n",
    "'''    \n",
    "def nlp_consumer(q):\n",
    "    nlp = StanfordCoreNLP('http://ec2-52-90-212-123.compute-1.amazonaws.com:9000/')\n",
    "    \n",
    "    final_list = []\n",
    "    while True: \n",
    "        text1 = q.get()\n",
    "        tmp_list = list(text1)\n",
    "        str_1 = ''.join(tmp_list)\n",
    "        ssplit = str_1.split()\n",
    "        \n",
    "        for word in ssplit:        \n",
    "            output = nlp.annotate(word, properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000,})\n",
    "            final_list.append((output['sentences'][0]['sentiment']))\n",
    "            \n",
    "        np_a = np.array(final_list)\n",
    "        for each in np_a:\n",
    "            unique_elements, counts_elements = np.unique(a, return_counts=True)\n",
    "        print(\"Frequency of unique values in array:\")\n",
    "        print(np.asarray((unique_elements, counts_elements)))\n",
    "   \n",
    "        if len(tmp_list) == 273:  #break condition\n",
    "            q.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of unique values of the said array:\n",
      "[['Negative' 'Neutral' 'Positive' 'Verynegative' 'Verypositive']\n",
      " ['36' '1218' '50' '6' '7']]\n",
      "Frequency of unique values of the said array:\n",
      "[['Negative' 'Neutral' 'Positive' 'Verynegative' 'Verypositive']\n",
      " ['65' '2716' '100' '10' '11']]\n",
      "run time:623.9797768592834\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''\n",
    "Multiprocessing and queue steps below.\n",
    "'''\n",
    "q = mp.Queue()\n",
    "p1 = mp.Process(name='scrape_text1', target=list_from_text, args=(data1, q))\n",
    "p2 = mp.Process(name='scrape_text2', target=list_from_text, args=(data2, q))\n",
    "p3 = mp.Process(name='nlp_consumer', target=nlp_consumer, args=(q,))\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "p3.start()\n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "p3.join()\n",
    "\n",
    "print(\"run time:\" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write-up\n",
    "\n",
    "For this assignment, I chose to take New York Times op-ed articles written by members of the Trump administration as my text stream -- and ultimately do sentiment analysis on the scraped text.  Doing so involved spinning up an Ubunutu ec2 instance with corenlp deployed (code for this is included at the top of this notebook), ingesting my chosen text stream, creating two producer processes that made use of my functions that scrape relevant article text, creating a consumer process that took items from the queue, parellelzing the processes, and using nlp functions to analyze my stream.  \n",
    "\n",
    "The first article in my stream is the now notorious and anonymous 'I Am Part of the Resistance Inside the Trump Administration' -- while the second article in my stream is titled 'My Year as a Trump Ambassador' written publicly by former US Ambassador to Mexico Roberta Jacobson. From the start, I was curious about how positive or negative the sentiments of these articles would be when discussing the environment of working for the Trump administration -- and if the anonymous first article was meaningfully different in sentiment than the second public article in the text stream.\n",
    "\n",
    "Without considering neutral words, the results of my sentiment analysis for the first anonymous article show that ~42% (42/99) of the words analyzed were negative or very negative, while ~58% (57/99) were positive or very positive.  In the second article, again without considering neutral words, the results of my analysis show that ~40% (75/186) of the words analyzed were negative or very negative, while ~60% (111/186) were positive or very positive.  The results on my chosen text stream are interesting to me because of how remarkably similar to each other across the two articles analyzed -- despite one being anonymous and the other being public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
