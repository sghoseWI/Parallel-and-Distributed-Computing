{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1.\n",
    "\n",
    "Write an AWS Command Line call to aws emr create-cluster, that:\n",
    "Launches a cluster of 3 m5.xlarge machines (one name, two worker nodes). Installs the applications of Spark, Ganglia, and JupyterHub; Adds an additional name node security group opening pertinent ports (JupyterHub - which is 9443, SSH, Spark History Server (Links to an external site.)Links to an external site.); Uses a key-pair for secure access; Runs a bootstrap script that you have stored in AWS S3 (code for that bootstrap script is below).\n",
    "\n",
    "### AWS CLI code for initializing cluster:\n",
    "\n",
    "aws emr create-cluster --auto-scaling-role EMR_AutoScaling_DefaultRole --applications Name=Hive Name=JupyterHub Name=Ganglia Name=Spark --ebs-root-volume-size 10 --ec2-attributes '{\"KeyName\":\"aws-key1\",\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-05f8732b\",\"EmrManagedSlaveSecurityGroup\":\"sg-0314eaf4b5c5d861d\",\"EmrManagedMasterSecurityGroup\":\"sg-0314eaf4b5c5d861d\"}' --service-role EMR_DefaultRole --enable-debugging --release-label emr-5.19.0 --log-uri 's3n://aws-logs-253161286339-us-east-1/elasticmapreduce/' --name 'lsdm_cluster_ps3_vf' --instance-groups '[{\"InstanceCount\":1,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":1}]},\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Master - 1\"},{\"InstanceCount\":2,\"EbsConfiguration\":{\"EbsBlockDeviceConfigs\":[{\"VolumeSpecification\":{\"SizeInGB\":32,\"VolumeType\":\"gp2\"},\"VolumesPerInstance\":1}]},\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m5.xlarge\",\"Name\":\"Core - 2\"}]' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-east-1 --bootstrap-actions Path=s3://lsdmghosevf/lsdm_ps3_bash.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1541988201363_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-81-200.ec2.internal:20888/proxy/application_1541988201363_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-83-129.ec2.internal:8042/node/containerlogs/container_1541988201363_0002_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "sc.setJobGroup(\"001\", \"PS3 Job1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the SparkSession.builder to set spark.executor.instances to 4, spark.executor.memory to '6G', spark.executor.cores and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.executor.instances\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.executor.memory\", '6G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.executor.cores\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in all three csv files into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = spark.read.option('header', 'true').option('inferSchema', 'true').csv('s3://lsdm-emr-util/lsdm-data/dime_recipients_all_1979_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = spark.read.option('header', 'true').option('inferSchema', 'true').csv('s3://lsdm-emr-util/lsdm-data/vote_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = spark.read.option('header', 'true').option('inferSchema', 'true').csv('s3://lsdm-emr-util/lsdm-data/text_db.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alter column names for all dataframes to replace `.` with `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"002\", \"PS3 Job2\")\n",
    "def alter_col_names(df):\n",
    "    newColList = list((col.replace('.', '_') for col in df.columns))\n",
    "    for oldCol, newCol in zip(df.columns, newColList):\n",
    "        df = df.withColumnRenamed(oldCol, newCol)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = alter_col_names(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = alter_col_names(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = alter_col_names(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[election: string, cycle: string, fecyear: string, Cand_ID: string, FEC_ID: string, NID: string, ICPSR: string, ICPSR2: string, bonica_rid: string, bonica_cid: bigint, name: string, lname: string, ffname: string, fname: string, mname: string, nname: string, title: string, suffix: string, party: string, state: string, seat: string, district: string, Incum_Chall: string, recipient_cfscore: double, contributor_cfscore: double, recipient_cfscore_dyn: double, dwnom1: double, dwnom2: double, ps_dwnom1: double, ps_dwnom2: double, dwdime: double, irt_cfscore: double, num_givers: int, num_givers_total: int, n_data_points_personal_donations: int, n_data_points_personal_donations_unq: int, cand_gender: string, total_disbursements: double, total_pc_contribs: string, contribs_from_candidate: double, unitemized: double, non_party_ind_exp_for: decimal(10,0), non_party_ind_exp_against: string, ind_exp_for: decimal(10,0), ind_exp_against: int, comm_cost_for: int, comm_cost_against: int, party_coord_exp: decimal(10,0), party_ind_exp_against: decimal(10,0), total_receipts: double, total_indiv_contrib: double, total_pac_contribs: double, ran_primary: int, ran_general: int, p_elec_stat: string, s_elec_stat: string, r_elec_stat: string, gen_elec_stat: string, gen_elect_pct: string, winner: string, district_partisanship: double, district_pres_vs: double, candStatus: string, recipient_type: string, igcat: string, comtype: string, nimsp_party: string, nimsp_candidate_ICO_code: string, nimsp_district: string, nimsp_office: string, nimsp_candidate_status: string, before_switch_ICPSR: int, after_switch_ICPSR: int, party_orig: string]"
     ]
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[vote_id: string, bill_id: string, bonica_rid: string, Name: string, party: int, sponsor: int, cosponsor: int, vote_date: string, vote_choice: int, vs_idealPoint: double, vs_cuttingpoint: double, vs_rcdir: int]"
     ]
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[doc_id: string, doc_type: string, bonica_rid: string, bill_id: string, sponsor_rid: string, page_id: string, congno: int, legis_body: string, training_set: int, date: timestamp, text: string, stemmed_text: string, doc_labels: string, tw_latent1: double, tw_abortion_and_social_conservatism: double, tw_agriculture: double, tw_banking_and_finance: double, tw_civil_rights: double, tw_congress_and_procedural: double, tw_crime: double, tw_defense_and_foreign_policy: double, tw_economy: double, tw_education: double, tw_energy: double, tw_environment: double, tw_fair_elections: double, tw_federal_agencies_and_gov_regulation: double, tw_guns: double, tw_healthcare: double, tw_higher_education: double, tw_immigration: double, tw_indian_affairs: double, tw_intelligence_and_surveillance: double, tw_labor: double, tw_law_courts_and_judges: double, tw_transportation: double, tw_veterans_affairs: double, tw_womens_issues: double]"
     ]
    }
   ],
   "source": [
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = d1.withColumnRenamed(\"bonica_rid\", \"bonica_rid_d1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = d2.withColumnRenamed(\"bonica_rid\", \"bonica_rid_d2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = d3.withColumnRenamed(\"bonica_rid\", \"bonica_rid_d3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge three (of the four possible) dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"003\", \"PS3 Job3\")\n",
    "merged_df = d1.join(d2, d1.bonica_rid_d1 == d2.bonica_rid_d2,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.join(d3, merged_df.bonica_rid_d2 == d3.bonica_rid_d3,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[election: string, cycle: string, fecyear: string, Cand_ID: string, FEC_ID: string, NID: string, ICPSR: string, ICPSR2: string, bonica_rid_d1: string, bonica_cid: bigint, name: string, lname: string, ffname: string, fname: string, mname: string, nname: string, title: string, suffix: string, party: string, state: string, seat: string, district: string, Incum_Chall: string, recipient_cfscore: double, contributor_cfscore: double, recipient_cfscore_dyn: double, dwnom1: double, dwnom2: double, ps_dwnom1: double, ps_dwnom2: double, dwdime: double, irt_cfscore: double, num_givers: int, num_givers_total: int, n_data_points_personal_donations: int, n_data_points_personal_donations_unq: int, cand_gender: string, total_disbursements: double, total_pc_contribs: string, contribs_from_candidate: double, unitemized: double, non_party_ind_exp_for: decimal(10,0), non_party_ind_exp_against: string, ind_exp_for: decimal(10,0), ind_exp_against: int, comm_cost_for: int, comm_cost_against: int, party_coord_exp: decimal(10,0), party_ind_exp_against: decimal(10,0), total_receipts: double, total_indiv_contrib: double, total_pac_contribs: double, ran_primary: int, ran_general: int, p_elec_stat: string, s_elec_stat: string, r_elec_stat: string, gen_elec_stat: string, gen_elect_pct: string, winner: string, district_partisanship: double, district_pres_vs: double, candStatus: string, recipient_type: string, igcat: string, comtype: string, nimsp_party: string, nimsp_candidate_ICO_code: string, nimsp_district: string, nimsp_office: string, nimsp_candidate_status: string, before_switch_ICPSR: int, after_switch_ICPSR: int, party_orig: string, vote_id: string, bill_id: string, bonica_rid_d2: string, Name: string, party: int, sponsor: int, cosponsor: int, vote_date: string, vote_choice: int, vs_idealPoint: double, vs_cuttingpoint: double, vs_rcdir: int, doc_id: string, doc_type: string, bonica_rid_d3: string, bill_id: string, sponsor_rid: string, page_id: string, congno: int, legis_body: string, training_set: int, date: timestamp, text: string, stemmed_text: string, doc_labels: string, tw_latent1: double, tw_abortion_and_social_conservatism: double, tw_agriculture: double, tw_banking_and_finance: double, tw_civil_rights: double, tw_congress_and_procedural: double, tw_crime: double, tw_defense_and_foreign_policy: double, tw_economy: double, tw_education: double, tw_energy: double, tw_environment: double, tw_fair_elections: double, tw_federal_agencies_and_gov_regulation: double, tw_guns: double, tw_healthcare: double, tw_higher_education: double, tw_immigration: double, tw_indian_affairs: double, tw_intelligence_and_surveillance: double, tw_labor: double, tw_law_courts_and_judges: double, tw_transportation: double, tw_veterans_affairs: double, tw_womens_issues: double]"
     ]
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the same grouped aggregation in PySpark and then again in Spark SQL. Run .explain() on the resulting aggregations - what does this tell you about the plans for the two dataframes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"004\", \"PS3 Jo4b\")\n",
    "from pyspark.sql.functions import count, avg\n",
    "g_df = merged_df.groupBy(\"election\", \"cycle\").agg(avg(\"total_disbursements\"), count(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------------+---------+\n",
      "|            election|cycle|avg(total_disbursements)| count(1)|\n",
      "+--------------------+-----+------------------------+---------+\n",
      "|                1996| 1996|                    null| 47665388|\n",
      "|                2018| 2018|                    null|   226107|\n",
      "|                2012| 2012|                    null|  8771662|\n",
      "|                  9\"|    C|                    null|        1|\n",
      "|            fdfd2016| 2016|       927530.2371189878|469952690|\n",
      "|            fdfd2014| 2014|      1745129.8572086587|513770614|\n",
      "|\",,128224,0,0,0,0...|    1|                    null|        1|\n",
      "|            fdfd2006| 2006|       1412034.249539594|512123250|\n",
      "|                1990| 1990|                    null|  7688242|\n",
      "|                2004| 2004|                    null| 29082686|\n",
      "+--------------------+-----+------------------------+---------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "g_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(10) HashAggregate(keys=[election#10, cycle#11], functions=[avg(total_disbursements#3053), count(1)])\n",
      "+- Exchange hashpartitioning(election#10, cycle#11, 200)\n",
      "   +- *(9) HashAggregate(keys=[election#10, cycle#11], functions=[partial_avg(total_disbursements#3053), partial_count(1)])\n",
      "      +- *(9) Project [election#10, cycle#11, total_disbursements#3053]\n",
      "         +- SortMergeJoin [bonica_rid_d2#7629], [bonica_rid_d3#7642], LeftOuter\n",
      "            :- *(6) Sort [bonica_rid_d2#7629 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(bonica_rid_d2#7629, 200)\n",
      "            :     +- *(5) Project [election#10, cycle#11, total_disbursements#3053, bonica_rid_d2#7629]\n",
      "            :        +- SortMergeJoin [bonica_rid_d1#7554], [bonica_rid_d2#7629], LeftOuter\n",
      "            :           :- *(2) Sort [bonica_rid_d1#7554 ASC NULLS FIRST], false, 0\n",
      "            :           :  +- Exchange hashpartitioning(bonica_rid_d1#7554, 200)\n",
      "            :           :     +- *(1) Project [election#10, cycle#11, bonica.rid#18 AS bonica_rid_d1#7554, total.disbursements#47 AS total_disbursements#3053]\n",
      "            :           :        +- *(1) FileScan csv [election#10,cycle#11,bonica.rid#18,total.disbursements#47] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/dime_recipients_all_1979_2014.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<election:string,cycle:string,bonica.rid:string,total.disbursements:double>\n",
      "            :           +- *(4) Sort [bonica_rid_d2#7629 ASC NULLS FIRST], false, 0\n",
      "            :              +- Exchange hashpartitioning(bonica_rid_d2#7629, 200)\n",
      "            :                 +- *(3) Project [bonica_rid#170 AS bonica_rid_d2#7629]\n",
      "            :                    +- *(3) FileScan csv [bonica_rid#170] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/vote_db.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<bonica_rid:string>\n",
      "            +- *(8) Sort [bonica_rid_d3#7642 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(bonica_rid_d3#7642, 200)\n",
      "                  +- *(7) Project [bonica.rid#204 AS bonica_rid_d3#7642]\n",
      "                     +- *(7) FileScan csv [bonica.rid#204] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/text_db.csv.gz], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<bonica.rid:string>"
     ]
    }
   ],
   "source": [
    "g_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView(\"merged_df\")\n",
    "spark_sql_df = spark.sql(\"SELECT election, cycle, avg(total_disbursements), count(*) from merged_df group by election, cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------------+---------+\n",
      "|            election|cycle|avg(total_disbursements)| count(1)|\n",
      "+--------------------+-----+------------------------+---------+\n",
      "|                2018| 2018|                    null|   226107|\n",
      "|                1996| 1996|                    null| 47665388|\n",
      "|                2012| 2012|                    null|  8771662|\n",
      "|                  9\"|    C|                    null|        1|\n",
      "|            fdfd2016| 2016|       927530.2371189882|469952690|\n",
      "|            fdfd2014| 2014|       1745129.857208658|513770614|\n",
      "|\",,128224,0,0,0,0...|    1|                    null|        1|\n",
      "|            fdfd2006| 2006|       1412034.249539594|512123250|\n",
      "|                1990| 1990|                    null|  7688242|\n",
      "|                2004| 2004|                    null| 29082686|\n",
      "+--------------------+-----+------------------------+---------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "spark_sql_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(10) HashAggregate(keys=[election#10, cycle#11], functions=[avg(total_disbursements#3053), count(1)])\n",
      "+- Exchange hashpartitioning(election#10, cycle#11, 200)\n",
      "   +- *(9) HashAggregate(keys=[election#10, cycle#11], functions=[partial_avg(total_disbursements#3053), partial_count(1)])\n",
      "      +- *(9) Project [election#10, cycle#11, total_disbursements#3053]\n",
      "         +- SortMergeJoin [bonica_rid_d2#7629], [bonica_rid_d3#7642], LeftOuter\n",
      "            :- *(6) Sort [bonica_rid_d2#7629 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(bonica_rid_d2#7629, 200)\n",
      "            :     +- *(5) Project [election#10, cycle#11, total_disbursements#3053, bonica_rid_d2#7629]\n",
      "            :        +- SortMergeJoin [bonica_rid_d1#7554], [bonica_rid_d2#7629], LeftOuter\n",
      "            :           :- *(2) Sort [bonica_rid_d1#7554 ASC NULLS FIRST], false, 0\n",
      "            :           :  +- Exchange hashpartitioning(bonica_rid_d1#7554, 200)\n",
      "            :           :     +- *(1) Project [election#10, cycle#11, bonica.rid#18 AS bonica_rid_d1#7554, total.disbursements#47 AS total_disbursements#3053]\n",
      "            :           :        +- *(1) FileScan csv [election#10,cycle#11,bonica.rid#18,total.disbursements#47] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/dime_recipients_all_1979_2014.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<election:string,cycle:string,bonica.rid:string,total.disbursements:double>\n",
      "            :           +- *(4) Sort [bonica_rid_d2#7629 ASC NULLS FIRST], false, 0\n",
      "            :              +- Exchange hashpartitioning(bonica_rid_d2#7629, 200)\n",
      "            :                 +- *(3) Project [bonica_rid#170 AS bonica_rid_d2#7629]\n",
      "            :                    +- *(3) FileScan csv [bonica_rid#170] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/vote_db.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<bonica_rid:string>\n",
      "            +- *(8) Sort [bonica_rid_d3#7642 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(bonica_rid_d3#7642, 200)\n",
      "                  +- *(7) Project [bonica.rid#204 AS bonica_rid_d3#7642]\n",
      "                     +- *(7) FileScan csv [bonica.rid#204] Batched: false, Format: CSV, Location: InMemoryFileIndex[s3://lsdm-emr-util/lsdm-data/text_db.csv.gz], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<bonica.rid:string>"
     ]
    }
   ],
   "source": [
    "spark_sql_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this tell you about the plans for the two dataframes?\n",
    "\n",
    "After running the two separate grouped aggregations and running .explain() on them, we get to see the physical plan of the grouped aggregation queries.  The physical plan is essentially the ordered set of steps that the relational system uses to execute its query and access data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a windowing aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"005\", \"PS5 Job\")\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"election\").orderBy(\"contributor_cfscore\").rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_df = merged_df.select(rank().over(w), min('total_disbursements').over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|RANK() OVER (PARTITION BY election ORDER BY contributor_cfscore ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|min(total_disbursements) OVER (PARTITION BY election ORDER BY contributor_cfscore ASC NULLS FIRST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                1|                                                                                                                                           816546.0|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "win_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a User-Defined-Function (UDF), using Python's nltk package to analyze the text data in the Congressional Text dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"006\", \"PS3 Job6\")\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /var/lib/livy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "True"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_count_text(col, phrase):   # Using this text analysis function to get the distribution of the counts of unique phrases in a dataframe column\n",
    "    txt = str(col)\n",
    "    txt = nltk.word_tokenize(txt)\n",
    "    txt = nltk.Text(txt)\n",
    "    print(txt.count(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "udf_nltk_count_text = udf(nltk_count_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.select(udf_nltk_count_text(col(\"legis_body\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_freq_legis(df):\n",
    "#     leg_list = df.select('legis_body').collect()\n",
    "#     print(FreqDist(leg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "# d3.select(udf_nltk_count_text(col(\"legis_body\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "# udf_list_freq_legis = udf(list_freq_legis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d3.select(udf_list_freq_legis()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect a sample or aggregation of the data to the name node as a Pandas dataframe and print some of the rows of the collected dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setJobGroup(\"007\", \"PS3 Job7\")\n",
    "sample_df = merged_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample = sample_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(election='2010', cycle='2010', fecyear='2010', Cand_ID='WV130349', FEC_ID=None, NID=None, ICPSR='WV1303492010', ICPSR2='WV130349', bonica_rid_d1='cand118439', bonica_cid=3822466047, name='tucker, gregory a', lname='tucker', ffname='gregory a', fname='gregory', mname='a', nname=None, title=None, suffix=None, party='100', state='WV', seat='state:upper', district='WV-11', Incum_Chall='C', recipient_cfscore=-0.022, contributor_cfscore=-0.187, recipient_cfscore_dyn=-0.07, dwnom1=None, dwnom2=None, ps_dwnom1=None, ps_dwnom2=None, dwdime=-0.266, irt_cfscore=None, num_givers=100, num_givers_total=143, n_data_points_personal_donations=12, n_data_points_personal_donations_unq=12, cand_gender='M', total_disbursements=None, total_pc_contribs='0', contribs_from_candidate=26200.0, unitemized=0.0, non_party_ind_exp_for=Decimal('0'), non_party_ind_exp_against='0', ind_exp_for=Decimal('0'), ind_exp_against=0, comm_cost_for=0, comm_cost_against=0, party_coord_exp=Decimal('0'), party_ind_exp_against=Decimal('0'), total_receipts=107576.0, total_indiv_contrib=48925.0, total_pac_contribs=30365.0, ran_primary=1, ran_general=1, p_elec_stat='W', s_elec_stat=None, r_elec_stat=None, gen_elec_stat='W', gen_elect_pct=None, winner='W', district_partisanship=None, district_pres_vs=None, candStatus=None, recipient_type='cand', igcat=None, comtype=None, nimsp_party='DEMOCRAT', nimsp_candidate_ICO_code='C', nimsp_district='011', nimsp_office='SENATE', nimsp_candidate_status='Won', before_switch_ICPSR=None, after_switch_ICPSR=None, party_orig='100', vote_id=None, bill_id=None, bonica_rid_d2=None, Name=None, party=None, sponsor=None, cosponsor=None, vote_date=None, vote_choice=None, vs_idealPoint=None, vs_cuttingpoint=None, vs_rcdir=None, doc_id=None, doc_type=None, bonica_rid_d3=None, bill_id=None, sponsor_rid=None, page_id=None, congno=None, legis_body=None, training_set=None, date=None, text=None, stemmed_text=None, doc_labels=None, tw_latent1=None, tw_abortion_and_social_conservatism=None, tw_agriculture=None, tw_banking_and_finance=None, tw_civil_rights=None, tw_congress_and_procedural=None, tw_crime=None, tw_defense_and_foreign_policy=None, tw_economy=None, tw_education=None, tw_energy=None, tw_environment=None, tw_fair_elections=None, tw_federal_agencies_and_gov_regulation=None, tw_guns=None, tw_healthcare=None, tw_higher_education=None, tw_immigration=None, tw_indian_affairs=None, tw_intelligence_and_surveillance=None, tw_labor=None, tw_law_courts_and_judges=None, tw_transportation=None, tw_veterans_affairs=None, tw_womens_issues=None), Row(election='fdfd1980', cycle='1980', fecyear='1978', Cand_ID='S8WY00072', FEC_ID='C00101774', NID='NS8WY00072', ICPSR='S8WY000721980', ICPSR2='S8WY00072', bonica_rid_d1='cand107439', bonica_cid=None, name='whitaker, raymond b.', lname='whitaker', ffname='raymond b.', fname='raymond', mname='b', nname=None, title=None, suffix=None, party='100', state='WY', seat='federal:senate', district='WYS78', Incum_Chall='C', recipient_cfscore=None, contributor_cfscore=None, recipient_cfscore_dyn=None, dwnom1=None, dwnom2=None, ps_dwnom1=None, ps_dwnom2=None, dwdime=None, irt_cfscore=None, num_givers=None, num_givers_total=None, n_data_points_personal_donations=0, n_data_points_personal_donations_unq=0, cand_gender='M', total_disbursements=11124953.0, total_pc_contribs='0', contribs_from_candidate=0.0, unitemized=0.0, non_party_ind_exp_for=Decimal('0'), non_party_ind_exp_against='0', ind_exp_for=Decimal('0'), ind_exp_against=0, comm_cost_for=0, comm_cost_against=0, party_coord_exp=Decimal('0'), party_ind_exp_against=Decimal('0'), total_receipts=0.0, total_indiv_contrib=0.0, total_pac_contribs=0.0, ran_primary=0, ran_general=0, p_elec_stat=None, s_elec_stat=None, r_elec_stat=None, gen_elec_stat=None, gen_elect_pct=None, winner=None, district_partisanship=None, district_pres_vs=None, candStatus='P', recipient_type='cand', igcat=None, comtype='S', nimsp_party=None, nimsp_candidate_ICO_code=None, nimsp_district=None, nimsp_office=None, nimsp_candidate_status=None, before_switch_ICPSR=None, after_switch_ICPSR=None, party_orig='100', vote_id=None, bill_id=None, bonica_rid_d2=None, Name=None, party=None, sponsor=None, cosponsor=None, vote_date=None, vote_choice=None, vs_idealPoint=None, vs_cuttingpoint=None, vs_rcdir=None, doc_id=None, doc_type=None, bonica_rid_d3=None, bill_id=None, sponsor_rid=None, page_id=None, congno=None, legis_body=None, training_set=None, date=None, text=None, stemmed_text=None, doc_labels=None, tw_latent1=None, tw_abortion_and_social_conservatism=None, tw_agriculture=None, tw_banking_and_finance=None, tw_civil_rights=None, tw_congress_and_procedural=None, tw_crime=None, tw_defense_and_foreign_policy=None, tw_economy=None, tw_education=None, tw_energy=None, tw_environment=None, tw_fair_elections=None, tw_federal_agencies_and_gov_regulation=None, tw_guns=None, tw_healthcare=None, tw_higher_education=None, tw_immigration=None, tw_indian_affairs=None, tw_intelligence_and_surveillance=None, tw_labor=None, tw_law_courts_and_judges=None, tw_transportation=None, tw_veterans_affairs=None, tw_womens_issues=None)]"
     ]
    }
   ],
   "source": [
    "print(plot_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Conceptual Analysis - Identify and describe the following:\n",
    "\n",
    "Referenced: https://jaceklaskowski.gitbooks.io/mastering-apache-spark/, https://stackoverflow.com/questions/34580662/what-does-stage-skipped-mean-in-apache-spark-web-ui, https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-whole-stage-codegen.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shuffle between two stages;\n",
    "\n",
    "In this case, the one shuffle (a process wherein there is a transfer of data between stages or sets of parallel tasks) -- data is moved and redistributed from one stage to the other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One narrow transformation\n",
    "\n",
    "One narrow transformation is essentially a function like map/filter that takes an RDD as its input and returns at least one RDD as its ouput -- all on a single partition.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One wide transformation\n",
    "\n",
    "One wide transformation is a transformation that is essentailly the result of applying groupbyKey or reducebyKey.  In wide transformations, the data that is used to do computations over lives across multiple partitions -- unlike with a narrow transformation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For one Spark job, use the DAG visualization in the Spark History app to examine the tasks and stages, then explain in detail (include a screenshot of the DAG)\n",
    "\n",
    "In the screenshot attached to the assignment submission, we can see that there were four stages that were skipped (stages 77, 78, 79, and 80) while Stage 81 was completed.  The stages were likely skipped because the data was retrieved from the cache memory and it was not necessary to execute it again in the stage.  In the completed Stage 81, there are two exchanges where an entire query is imputed into a single function ready for execution.  Then, in the same stage, there are two WholeStageCodegen steps wherein physical operators are condensed into a single Java function.  From there, a SortMergeJoin is done to join back the relevant data toether.  At this point, mapPartitionsInternal is used in the completed Stage 81 to filter the data.  Finally, there is a map operation in this stage of the job.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the storage level of your dataframe. What happens if you checkpoint the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you checkpoint the data, for example with reliable checkpointing, the current state of the RDD is saved in a distributed file system like the Hadoop Distributed File System.  Further, the storage level of my dataframe goes down when I checkpoint the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
